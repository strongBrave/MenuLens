"""æ··åˆ RAG Pipeline - Search-Verify-Generate"""

import asyncio
import logging
import aiohttp
import time
from typing import List, Optional, Tuple, Any

from schemas import Dish
from config import settings
from .image_verifier import image_verifier
from .image_generator import image_generator

logger = logging.getLogger(__name__)


class HybridImagePipeline:
    """
    Search-Verify-Generate æ··åˆ Pipeline
    """
    
    def __init__(self, searcher, search_service):
        self.searcher = searcher
        self.search_service = search_service
        self.verifier = image_verifier
        self.generator = image_generator
    
    async def get_best_images(self, dish: Dish) -> Tuple[List[str], List[int]]:
        """
        è·å–èœå“çš„æœ€ä½³å›¾ç‰‡åˆ—è¡¨å’Œåˆ†æ•°åˆ—è¡¨
        
        Returns:
            (å›¾ç‰‡ URL åˆ—è¡¨, åˆ†æ•°åˆ—è¡¨)
        """
        if not settings.ENABLE_RAG_PIPELINE:
            logger.info(f"RAG Pipeline disabled, using legacy search for {dish.english_name}")
            return [], []
        
        start_time = time.time()
        logger.info(f"ğŸ” Pipeline START for {dish.english_name}")
        
        # Step 1: æœç´¢å¤šä¸ªå€™é€‰å›¾ç‰‡
        search_start = time.time()
        candidate_urls = await self._search_candidates(dish)
        search_time = time.time() - search_start
        
        if not candidate_urls:
            logger.warning(f"âš ï¸  No search results for {dish.english_name} ({search_time:.1f}s), skipping to generation")
            gen_img = await self._generate_image(dish)
            return ([gen_img], [99]) if gen_img else ([], [])
        
        logger.info(f"ğŸ“‹ Found {len(candidate_urls)} candidates ({search_time:.1f}s)")
        
        # Step 2: éªŒè¯å¹¶æ’åºå€™é€‰å›¾ç‰‡
        verify_start = time.time()
        sorted_results = await self._verify_and_sort(dish, candidate_urls)
        verify_time = time.time() - verify_start
        
        if sorted_results:
            total_time = time.time() - start_time
            # æå– URLs å’Œ åˆ†æ•°åˆ—è¡¨
            sorted_urls = [url for url, _ in sorted_results]
            sorted_scores = [int(score * 100) for _, score in sorted_results]
            
            logger.info(f"âœ… Found {len(sorted_urls)} verified images (Top: {sorted_scores[0]}%) ({verify_time:.1f}s verification, {total_time:.1f}s total)")
            return sorted_urls, sorted_scores
        
        # Step 3: éªŒè¯å¤±è´¥ï¼Œé™çº§ä¸ºç”Ÿæˆ
        logger.warning(f"âš ï¸  No valid search result (Score < {settings.IMAGE_VERIFY_SCORE_THRESHOLD}), "
                      f"generating image ({verify_time:.1f}s verification)")
        gen_img = await self._generate_image(dish)
        return ([gen_img], [99]) if gen_img else ([], [])

    async def _verify_and_sort(
        self,
        dish: Dish,
        candidate_urls: List[str]
    ) -> List[Tuple[str, float]]:
        """
        è§†è§‰éªŒè¯å¹¶æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åºå›¾ç‰‡
        Returns: List[(url, score)]
        """
        if not candidate_urls:
            return []
        
        # åªæœ‰ 1 ä¸ªç»“æœæ—¶è·³è¿‡å¤æ‚éªŒè¯ï¼ˆå¤ªæ…¢ï¼‰ï¼Œç›´æ¥è¿”å› mock score
        if len(candidate_urls) < 2:
            return [(candidate_urls[0], 0.85)]

        logger.info(f"ğŸ” Verifying {len(candidate_urls)} images...")
        
        # å¹¶å‘éªŒè¯æ‰€æœ‰å€™é€‰å›¾ç‰‡
        verification_tasks = [
            self.verifier.verify_image_relevance(
                dish_name=dish.english_name,
                description=dish.description,
                image_url=url,
                original_name=dish.original_name
            )
            for url in candidate_urls
        ]
        
        scores = await asyncio.gather(*verification_tasks, return_exceptions=True)
        
        # é…å¯¹ URL å’Œåˆ†æ•°
        valid_scored_urls = []
        for url, score in zip(candidate_urls, scores):
            if isinstance(score, (int, float)):
                # è®°å½•åˆ†æ•°æ—¥å¿—
                logger.debug(f"  {dish.english_name}: {score:.2f} - {url[:50]}...")
                if score >= 0.4: # é˜ˆå€¼
                    valid_scored_urls.append((url, score))
        
        if not valid_scored_urls:
            return []
        
        # æŒ‰åˆ†æ•°é™åºæ’åº
        valid_scored_urls.sort(key=lambda x: x[1], reverse=True)
        
        return valid_scored_urls

    async def _search_candidates(self, dish: Dish) -> List[str]:
        """æœç´¢å‰ N ä¸ªå€™é€‰å›¾ç‰‡"""
        try:
            # ä½¿ç”¨æœç´¢æœåŠ¡è·å–å¤šä¸ªç»“æœ
            urls = await self.search_service.search_images(
                dish.search_term,
                num=settings.SEARCH_CANDIDATE_RESULTS
            )
            
            if not urls:
                return []
            
            # å¿«é€Ÿæ£€æŸ¥ URL æœ‰æ•ˆæ€§ï¼ˆå‘é€ HEAD è¯·æ±‚ï¼‰
            valid_urls = await self._check_urls_alive(urls)
            extracted_num = min(len(valid_urls), settings.SEARCH_CANDIDATE_RESULTS)
            valid_urls = valid_urls[:extracted_num]
            logger.info(f"URL validity check: {len(valid_urls)}/{len(urls)} alive for {dish.english_name}")
            
            return valid_urls
            
        except Exception as e:
            logger.error(f"Error searching candidates for {dish.english_name}: {str(e)}")
            return []
    
    async def _check_urls_alive(self, urls: List[str], timeout: int = None) -> List[str]:
        """
        æ‰¹é‡æ£€æŸ¥ URL æ˜¯å¦å­˜æ´»ä¸”æ˜¯çœŸæ­£çš„å›¾ç‰‡
        """
        if timeout is None:
            timeout = settings.IMAGE_URL_CHECK_TIMEOUT
        
        VALID_IMAGE_TYPES = {
            'image/jpeg', 'image/jpg', 'image/png', 'image/webp', 'image/gif'
        }
        
        async def check_single_url(url: str) -> Optional[str]:
            try:
                timeout_obj = aiohttp.ClientTimeout(total=timeout)
                async with aiohttp.ClientSession() as session:
                    async with session.head(url, timeout=timeout_obj, allow_redirects=True) as resp:
                        if resp.status >= 400:
                            return None
                        
                        content_type = resp.headers.get('content-type', '').lower()
                        base_type = content_type.split(';')[0].strip()
                        
                        if base_type not in VALID_IMAGE_TYPES:
                            return None
                        
                        return url
                        
            except Exception:
                return None
        
        tasks = [check_single_url(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return [url for url in results if isinstance(url, str)]

    async def _generate_image(self, dish: Dish) -> Optional[str]:
        """é™çº§ï¼šç”Ÿæˆå›¾ç‰‡"""
        if not settings.ENABLE_IMAGE_GENERATION:
            return None
        
        logger.info(f"ğŸ¨ Generating image for {dish.english_name}...")
        
        try:
            image_url = await self.generator.generate_image(
                english_name=dish.english_name,
                original_name=dish.original_name,
                description=dish.description
            )
            return image_url
        except Exception as e:
            logger.error(f"Error generating image: {str(e)}")
            return None

    async def enrich_dishes_with_images(self, dishes: List[Dish]) -> List[Dish]:
        """
        ä¸ºèœå“åˆ—è¡¨å¹¶å‘è·å–æœ€ä½³å›¾ç‰‡ï¼ˆä½¿ç”¨æ··åˆ Pipelineï¼‰
        """
        logger.info(f"ğŸš€ Hybrid Pipeline processing {len(dishes)} dishes...")
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰èœå“
        tasks = [self.get_best_images(dish) for dish in dishes]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ›´æ–°èœå“å›¾ç‰‡
        success_count = 0
        for dish, result in zip(dishes, results):
            if isinstance(result, tuple) and result[0]:
                image_urls, image_scores = result
                dish.image_urls = image_urls
                dish.image_scores = image_scores # å­˜å‚¨æ‰€æœ‰åˆ†æ•°
                dish.image_url = image_urls[0] 
                dish.match_score = image_scores[0] # æœ€ä½³åˆ†æ•° (å…¼å®¹æ—§å­—æ®µ)
                success_count += 1
            elif isinstance(result, Exception):
                logger.warning(f"Exception for {dish.english_name}: {result}")
        
        logger.info(f"âœ… Pipeline completed: {success_count}/{len(dishes)} dishes got images")
        return dishes


# å…¨å±€å®ä¾‹ï¼ˆåœ¨ main.py ä¸­åˆå§‹åŒ–ï¼‰
hybrid_pipeline = None


def initialize_hybrid_pipeline(searcher, search_service):
    """åˆå§‹åŒ–å…¨å±€ Pipeline å®ä¾‹"""
    global hybrid_pipeline
    hybrid_pipeline = HybridImagePipeline(searcher, search_service)
    logger.info("âœ… Hybrid Pipeline initialized")
    return hybrid_pipeline
