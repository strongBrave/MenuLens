"""æ··åˆ RAG Pipeline - Search-Verify-Generate"""

import asyncio
import logging
import aiohttp
import time
from typing import List, Optional

from schemas import Dish
from config import settings
from .image_verifier import image_verifier
from .image_generator import image_generator

logger = logging.getLogger(__name__)


class HybridImagePipeline:
    """
    Search-Verify-Generate æ··åˆ Pipeline
    
    æ ¸å¿ƒé€»è¾‘ï¼š
    1. æœç´¢ï¼šè·å– Top 3 ç»“æœï¼ˆè€Œéä»… Top 1ï¼‰
    2. éªŒè¯ï¼šé€šè¿‡è§†è§‰æ¨¡å‹éªŒè¯ç›¸å…³æ€§ï¼ˆScore > 0.7ï¼‰
    3. ç”Ÿæˆï¼šéªŒè¯å¤±è´¥åˆ™è°ƒç”¨å›¾ç‰‡ç”Ÿæˆæ¨¡å‹
    """
    
    def __init__(self, searcher, search_service):
        """
        åˆå§‹åŒ– Pipeline
        
        Args:
            searcher: GoogleSearcher å®ä¾‹ï¼ˆå·²é…ç½®çš„æœç´¢æœåŠ¡ï¼‰
            search_service: å®Œæ•´çš„æœç´¢æœåŠ¡ï¼ˆåŒ…å«å¤šç»“æœè·å–ï¼‰
        """
        self.searcher = searcher
        self.search_service = search_service
        self.verifier = image_verifier
        self.generator = image_generator
    
    async def get_best_images(self, dish: Dish) -> List[str]:
        """
        è·å–èœå“çš„æœ€ä½³å›¾ç‰‡åˆ—è¡¨ï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼‰
        
        æ‰§è¡Œæµç¨‹ï¼š
        1. æœç´¢ Top N å›¾ç‰‡
        2. æ£€æŸ¥ URL æœ‰æ•ˆæ€§
        3. è§†è§‰éªŒè¯å¹¶æ‰“åˆ†
        4. æŒ‰åˆ†æ•°æ’åºè¿”å› Top 3
        5. å¦‚æœéªŒè¯å¤±è´¥ä¸”å…è®¸ç”Ÿæˆï¼Œåˆ™è¿”å›ç”Ÿæˆå›¾ç‰‡çš„åˆ—è¡¨
        
        Returns:
            å›¾ç‰‡ URL åˆ—è¡¨
        """
        if not settings.ENABLE_RAG_PIPELINE:
            logger.info(f"RAG Pipeline disabled, using legacy search for {dish.english_name}")
            return []
        
        start_time = time.time()
        logger.info(f"ğŸ” Pipeline START for {dish.english_name}")
        
        # Step 1: æœç´¢å¤šä¸ªå€™é€‰å›¾ç‰‡
        search_start = time.time()
        candidate_urls = await self._search_candidates(dish)
        search_time = time.time() - search_start
        
        if not candidate_urls:
            logger.warning(f"âš ï¸  No search results for {dish.english_name} ({search_time:.1f}s), skipping to generation")
            gen_img = await self._generate_image(dish)
            return [gen_img] if gen_img else []
        
        logger.info(f"ğŸ“‹ Found {len(candidate_urls)} candidates ({search_time:.1f}s)")
        
        # Step 2: éªŒè¯å¹¶æ’åºå€™é€‰å›¾ç‰‡
        verify_start = time.time()
        sorted_urls = await self._verify_and_sort(dish, candidate_urls)
        verify_time = time.time() - verify_start
        
        if sorted_urls:
            total_time = time.time() - start_time
            logger.info(f"âœ… Found {len(sorted_urls)} verified images ({verify_time:.1f}s verification, {total_time:.1f}s total) for {dish.english_name}")
            return sorted_urls
        
        # Step 3: éªŒè¯å¤±è´¥ï¼Œé™çº§ä¸ºç”Ÿæˆ
        logger.warning(f"âš ï¸  No valid search result (Score < {settings.IMAGE_VERIFY_SCORE_THRESHOLD}), "
                      f"generating image ({verify_time:.1f}s verification)")
        gen_img = await self._generate_image(dish)
        return [gen_img] if gen_img else []

    async def _verify_and_sort(
        self,
        dish: Dish,
        candidate_urls: List[str]
    ) -> List[str]:
        """
        è§†è§‰éªŒè¯å¹¶æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åºå›¾ç‰‡
        """
        if not candidate_urls:
            return []
        
        # åªæœ‰ 1 ä¸ªç»“æœæ—¶è·³è¿‡å¤æ‚éªŒè¯ï¼ˆå¤ªæ…¢ï¼‰ï¼Œç›´æ¥è¿”å›
        if len(candidate_urls) < 2:
            return candidate_urls

        logger.info(f"ğŸ” Verifying {len(candidate_urls)} images...")
        
        # å¹¶å‘éªŒè¯æ‰€æœ‰å€™é€‰å›¾ç‰‡
        verification_tasks = [
            self.verifier.verify_image_relevance(
                dish_name=dish.english_name,
                description=dish.description,
                image_url=url,
                original_name=dish.original_name
            )
            for url in candidate_urls
        ]
        
        scores = await asyncio.gather(*verification_tasks, return_exceptions=True)
        
        # é…å¯¹ URL å’Œåˆ†æ•°
        valid_scored_urls = []
        for url, score in zip(candidate_urls, scores):
            if isinstance(score, (int, float)):
                # è®°å½•åˆ†æ•°æ—¥å¿—
                logger.debug(f"  {dish.english_name}: {score:.2f} - {url[:50]}...")
                # å³ä½¿åˆ†æ•°ä½ä¹Ÿå…ˆä¿ç•™ï¼ŒæŒ‰åˆ†æ•°æ’åºï¼ˆé™¤ééå¸¸ç¦»è°±ï¼Œè¿™é‡Œæˆ‘ä»¬ä¿¡ä»» search çš„åŸºæœ¬ç›¸å…³æ€§ï¼‰
                # æˆ–è€…æˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸€ä¸ªç¡¬é˜ˆå€¼è¿‡æ»¤
                if score >= 0.4: # ç¨å¾®æ”¾å®½ä¸€ç‚¹é˜ˆå€¼ï¼Œä¿è¯æœ‰ç»“æœè¿”å›ï¼Œæ’åºé å‰çš„è‚¯å®šæ˜¯å¥½çš„
                    valid_scored_urls.append((url, score))
        
        if not valid_scored_urls:
            return []
        
        # æŒ‰åˆ†æ•°é™åºæ’åº
        valid_scored_urls.sort(key=lambda x: x[1], reverse=True)
        
        # è¿”å›æ’åºåçš„ URL åˆ—è¡¨
        return [url for url, _ in valid_scored_urls]

    # ... (Keep existing methods: _search_candidates, _check_urls_alive, _generate_image)
    async def _search_candidates(self, dish: Dish) -> List[str]:
        """æœç´¢å‰ N ä¸ªå€™é€‰å›¾ç‰‡"""
        try:
            # ä½¿ç”¨æœç´¢æœåŠ¡è·å–å¤šä¸ªç»“æœ
            urls = await self.search_service.search_images(
                dish.search_term,
                num=settings.SEARCH_CANDIDATE_RESULTS
            )
            
            if not urls:
                return []
            
            # å¿«é€Ÿæ£€æŸ¥ URL æœ‰æ•ˆæ€§ï¼ˆå‘é€ HEAD è¯·æ±‚ï¼‰
            valid_urls = await self._check_urls_alive(urls)
            extracted_num = min(len(valid_urls), settings.SEARCH_CANDIDATE_RESULTS)
            valid_urls = valid_urls[:extracted_num]
            logger.info(f"URL validity check: {len(valid_urls)}/{len(urls)} alive for {dish.english_name}")
            
            return valid_urls
            
        except Exception as e:
            logger.error(f"Error searching candidates for {dish.english_name}: {str(e)}")
            return []
    
    async def _check_urls_alive(self, urls: List[str], timeout: int = None) -> List[str]:
        """
        æ‰¹é‡æ£€æŸ¥ URL æ˜¯å¦å­˜æ´»ä¸”æ˜¯çœŸæ­£çš„å›¾ç‰‡
        
        éªŒè¯é¡¹ï¼š
        1. HTTP çŠ¶æ€ç  < 400
        2. Content-Type å¿…é¡»æ˜¯å›¾ç‰‡ç±»å‹ï¼ˆimage/jpeg, image/png, image/webpï¼‰
        3. æ’é™¤ HTML é‡å®šå‘/é”™è¯¯é¡µé¢
        """
        if timeout is None:
            timeout = settings.IMAGE_URL_CHECK_TIMEOUT
        
        VALID_IMAGE_TYPES = {
            'image/jpeg', 'image/jpg', 'image/png', 'image/webp', 'image/gif'
        }
        
        async def check_single_url(url: str) -> Optional[str]:
            try:
                timeout_obj = aiohttp.ClientTimeout(total=timeout)
                async with aiohttp.ClientSession() as session:
                    async with session.head(url, timeout=timeout_obj, allow_redirects=True) as resp:
                        if resp.status >= 400:
                            logger.debug(f"URL check failed ({resp.status}): {url[:50]}...")
                            return None
                        
                        # æ£€æŸ¥ Content-Type æ˜¯å¦æ˜¯å›¾ç‰‡
                        content_type = resp.headers.get('content-type', '').lower()
                        # æå–ä¸»ç±»å‹ï¼ˆå¤„ç† "image/jpeg; charset=utf-8" çš„æƒ…å†µï¼‰
                        base_type = content_type.split(';')[0].strip()
                        
                        if base_type not in VALID_IMAGE_TYPES:
                            logger.debug(f"Invalid content-type ({base_type}): {url[:50]}...")
                            return None
                        
                        return url
                        
            except asyncio.TimeoutError:
                logger.debug(f"URL check timeout: {url[:50]}...")
                return None
            except Exception as e:
                logger.debug(f"URL check error: {type(e).__name__}")
                return None
        
        # å¹¶å‘æ£€æŸ¥æ‰€æœ‰ URL
        tasks = [check_single_url(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤æœ‰æ•ˆçš„ URL
        valid_urls = [url for url in results if isinstance(url, str)]
        return valid_urls

    async def _generate_image(self, dish: Dish) -> Optional[str]:
        """é™çº§ï¼šç”Ÿæˆå›¾ç‰‡"""
        if not settings.ENABLE_IMAGE_GENERATION:
            logger.warning(f"Image generation disabled, returning None for {dish.english_name}")
            return None
        
        logger.info(f"ğŸ¨ Generating image for {dish.english_name}...")
        
        try:
            image_url = await self.generator.generate_image(
                english_name=dish.english_name,
                original_name=dish.original_name,
                description=dish.description
            )
            
            if image_url:
                logger.info(f"âœ¨ Generated image URL: {image_url[:60]}...")
            
            return image_url
            
        except Exception as e:
            logger.error(f"Error generating image: {str(e)}")
            return None

    async def enrich_dishes_with_images(self, dishes: List[Dish]) -> List[Dish]:
        """
        ä¸ºèœå“åˆ—è¡¨å¹¶å‘è·å–æœ€ä½³å›¾ç‰‡ï¼ˆä½¿ç”¨æ··åˆ Pipelineï¼‰
        
        Args:
            dishes: èœå“åˆ—è¡¨
            
        Returns:
            å¸¦æœ‰å›¾ç‰‡åˆ—è¡¨çš„èœå“åˆ—è¡¨
        """
        logger.info(f"ğŸš€ Hybrid Pipeline processing {len(dishes)} dishes...")
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰èœå“
        tasks = [self.get_best_images(dish) for dish in dishes]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ›´æ–°èœå“å›¾ç‰‡
        success_count = 0
        for dish, image_urls in zip(dishes, results):
            if isinstance(image_urls, list) and image_urls:
                dish.image_urls = image_urls
                dish.image_url = image_urls[0] # è®¾ç½®æœ€ä½³å›¾ç‰‡ä¸ºä¸»å›¾
                success_count += 1
            elif isinstance(image_urls, Exception):
                logger.warning(f"Exception for {dish.english_name}: {image_urls}")
        
        logger.info(f"âœ… Pipeline completed: {success_count}/{len(dishes)} dishes got images")
        return dishes


# å…¨å±€å®ä¾‹ï¼ˆåœ¨ main.py ä¸­åˆå§‹åŒ–ï¼‰
hybrid_pipeline = None


def initialize_hybrid_pipeline(searcher, search_service):
    """åˆå§‹åŒ–å…¨å±€ Pipeline å®ä¾‹"""
    global hybrid_pipeline
    hybrid_pipeline = HybridImagePipeline(searcher, search_service)
    logger.info("âœ… Hybrid Pipeline initialized")
    return hybrid_pipeline
